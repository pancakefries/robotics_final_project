# robotics_final_project
## Project Description
The goal of this project is to create a game that is in the spirit of capture the flag, involving playing against an opponent to get a flag to a goal and tagging an opponent to reset their position. The hope is to create an interactive and fun game that provides a challenge to both the player and the robot. 

The main components of the project are the pathfinding algorithm and the robot controller. The pathfinding algorithm receives starting and ending positions from the robot controller, which it then path finds between using data from the map created using SLAM. The robot controller receives this path and navigates the robot along it for a short period, due to the constantly updating position of the player. Once the robot reaches the flag it will pick it up, and once it reaches the goal it will set it down. 

## System Architecture
``pathfinding.py`` contains the implementation of the A* pathfinding algorithm. This script has rospy subscribers and publishers that subscribe to ``/target_nodes``, which provides the starting and ending coordinates which the algorithm should find a path between, and that publish to ``/target_path``, which provides the path generated by A*. ``pathfinding.py`` also subscribes to ``/map`` in order to incorporate obstacle data into its generated paths.

``robot_controller.py`` contains the bulk of the logic for this project. It subscribes to ``/target_path``, ``/camera/rgb/image_raw``, ``/scan``, ``/amcl_pose``, and ``/map``. It publishes to ``/target_nodes`` and ``/cmd_vel``. Upon receiving a path from ``/target_path`` it will update the current goal to reflect the new path and follow that path for up to 5 grid cells. The main loop decides a current target by computing whether the robot is a shorter distance from the player or its goal and then updates its movement accordingly. At each movement update, the script determines if the bot is at its current goal (end goal, player, or flag). If it is, the main loop pauses and the camera and LiDAR callback functions update the robot movement to approach the flag or goal. Once it is aligned properly, the robot then picks up or places the flag accordingly. 

## Execution
In a terminal window on your PC, run:
``roscore``

If using gazebo, run:
``roslaunch robotics_final_project turtlebot3_manipulation_gazebo.launch``

If using physical turtlebots, run BOTH of the following bringup commands in new windows:
``ssh pi@192.168.0.1xx``
``bringup_cam``

``ssh pi@192.168.0.1xx``
``bringup``

In a new terminal back on your PC, run:
``roslaunch robotics_final_project map_navigation_psu.launch``

If using the arm (i.e. not testing pathfinding), in another terminal, run:
``roslaunch turtlebot3_manipulation_moveit move_group.launch``

In another terminal run:
``rosrun robotics_final_project pathfinding.py``

And finally, run the following in a new terminal:
``rosrun robotics_final_project robot_controller.py``

## Challenges
The biggest challenges in this project were the integration of obstacles into pathfinding and getting the localization to work properly.	Although the pathfinding algorithm worked well in a blank grid world, getting it to work properly while navigating around obstacles presented a lot of difficulty. I attempted two solutions for this, one which involved downsampling the map image and the other trying to “pad” the obstacles in the map. Neither solution fully corrected the problem, though I think the solution that padded the map got the closest. My thinking was that the paths found were hugging the walls, not accounting for the width of the robot. For getting the localization to work properly, there were several factors that needed to be taken into account, namely the correct scaling of coordinates given by ``/amcl_pose`` that would translate correctly to the map coordinates, as well as calculating the correct offsets for the scaled coordinates to map to the correct locations. The scaling ended up just being the map resolution sent along with the map in the ``/map`` topic, and the offsets were calculated using image editing software and counting pixels to calibrate the default location of the robot.
 ## Future Work
If I had more time to work on this project I would dedicate much of it to debugging the pathfinding algorithm, as it still has some difficulties navigating around obstacles. I would also work on calibrating the pickup and placement of the flags, because although the code for doing such exists in ``robot_controller.py``, I have yet to properly calibrate the approach threshold due to the lacking ability to path find around obstacles. Additionally, I would better support using multiple turtlebots at once, as well as properly implement the localization of the player robot in addition to the robot (I believe doing multiple localizations requires multiple computers, and as a group of just myself this was not feasible to implement). 

## Takeaways
- I have come away with a much better understanding of how the A* algorithm works. Having to implement the algorithm from scratch forced me to have a good understanding of what was required for it to work.
- I feel like this project in particular has given me a vastly better understanding of how to design and implement a complex system with lots of moving parts. Having multiple scripts sending and receiving messages from each other while also being updated with sensor information and pose estimation was a tough challenge to both design and implement. I have never had to do anything close to this, much less on my own, and I feel like this has been a very practically applicable and valuable experience to have had (even if not all the features I would’ve liked to have were implemented).

## Demo
![trim.D35D97A0-B768-4A02-A640-0E239C5AFBAE.gif]https://github.com/pancakefries/robotics_final_project/blob/main/trim.D35D97A0-B768-4A02-A640-0E239C5AFBAE.gif
